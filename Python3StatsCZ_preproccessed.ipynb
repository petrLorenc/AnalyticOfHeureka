{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load data\n",
    "with open('./json/file_new_stopwords_token.json') as data_file:\n",
    "    dataNormalPre = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[('zbozi', 2), ('zbooi', 1)]\n"
     ]
    }
   ],
   "source": [
    "count_v = CountVectorizer()\n",
    "wo = count_v.fit_transform([\"zbozi zboOi\",\"zbozi\"])\n",
    "print (len (count_v.vocabulary_.items()))\n",
    "# most common words\n",
    "freqs = [(word, wo.getcol(idx).sum()) for word, idx in count_v.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "print (sorted (freqs, key = lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load right dataset\n",
    "dataIteration = dataNormalPre\n",
    "\n",
    "# for get ridding of non czech/english words\n",
    "def filterNonWordsOut(dataIteration):\n",
    "    dataIteration_preprocess = []\n",
    "\n",
    "    words_order = dataIteration\n",
    "    only_english_words = []\n",
    "\n",
    "    # to keep only english words\n",
    "    with open(\"./wordlist_cz.txt\") as word_file:\n",
    "        only_english_words = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "    for line in words_order:\n",
    "        english_line = \"\"\n",
    "        for w in line.split():\n",
    "            if w.strip().lower() in english_words_file:\n",
    "                english_line += str(w.strip().lower()) + \" \"\n",
    "            else: \n",
    "                pass\n",
    "        dataIteration_preprocess.append(english_line)    \n",
    "    return dataIteration_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#divide data into two parts -> plus/minus\n",
    "\n",
    "words = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "\n",
    "#every review\n",
    "for x in range(0,len(dataIteration)):\n",
    "\n",
    "    \n",
    "    for plus in  dataIteration[x].get(\"plus\"):\n",
    "        words.append(plus)\n",
    "        labels.append(1)\n",
    "        \n",
    "\n",
    "    for minus in dataIteration[x].get(\"minus\"):\n",
    "        words.append(minus)\n",
    "        labels.append(0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(words, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "words_train_count = count_vect.fit_transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18599"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (count_vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cena', 4147), ('rychlý', 2731), ('dobrý', 2095), ('zboží', 1682), ('žádný', 1367), ('vědět', 1123), ('kvalita', 953), ('zatím', 921), ('kvalitní', 859), ('komunikace', 830)]\n"
     ]
    }
   ],
   "source": [
    "# most common words\n",
    "freqs = [(word, words_train_count.getcol(idx).sum()) for word, idx in count_vect.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "print (sorted (freqs, key = lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(words_train_count)\n",
    "words_train_tf = tf_transformer.transform(words_train_count)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "words_train_tfidf = tfidf_transformer.fit_transform(words_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(words_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.618511918022\n",
      "0.713700576778\n",
      "0.571100246674\n",
      "0.618511918022\n",
      "0.571004946832\n",
      "[ 0.42849992  0.71370058]\n"
     ]
    }
   ],
   "source": [
    "docs_new = X_test\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "print(\"Score \" + str(clf.score(X_new_tfidf, y_test)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "# F1 = 2 * (precision * recall) / (precision + recall) \n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"binary\")) ## binary is most suitable for us -> 0/1\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"macro\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"micro\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"weighted\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC :0.847172156058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "predicted_proba = clf.predict_proba(X_new_tfidf)\n",
    "# Compute Area Under the Curve (AUC) from prediction scores\n",
    "print (\"ROC :\" + str(roc_auc_score(y_test, predicted_proba[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__n_iter': (10, 50, 80), 'clf__penalty': ('l2', 'elasticnet')}\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:   11.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 11.939s\n",
      "\n",
      "Best score: 0.749\n",
      "Best parameters set:\n",
      "\tclf__n_iter: 10\n",
      "\tclf__penalty: 'l2'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample pipeline for text feature extraction and evaluation\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', max_df=0.5, stop_words=None, max_features=None)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, norm=\"l1\")),\n",
    "    ('clf', SGDClassifier()),\n",
    "    #    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "with open('./czech_stop_words.txt') as stop_words:\n",
    "    stop_word_list = list([f.strip() for f in stop_words])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "# parameters = {\n",
    "#     'vect__max_df': (0.5, 0.75, 1.0),\n",
    "#     'vect__stop_words' : (stop_word_list ,'english', None),\n",
    "#     'vect__max_features': (None, 5000, 10000, 50000),\n",
    "#     'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "#     'vect__analyzer': ('word', 'char', 'char_wb') ,\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'tfidf__norm': ('l1', 'l2'),\n",
    "#     #'clf__penalty': ('l2', 'elasticnet'),\n",
    "#     #'clf__n_iter': (10, 50, 80),\n",
    "# }\n",
    "\n",
    "parameters = {\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "# find the best parameters for both the feature extraction and the\n",
    "# classifier\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(words, labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len :(35908, 1000)\n",
      "Feat : 1000\tScore :0.60793049677\t f1 : 0.70802919708\t ROC : 0.832492767645\n",
      "Len :(35908, 2000)\n",
      "Feat : 2000\tScore :0.614947649811\t f1 : 0.711844627824\t ROC : 0.840357643787\n",
      "Len :(35908, 3000)\n",
      "Feat : 3000\tScore :0.616952550679\t f1 : 0.714012474012\t ROC : 0.844476448744\n",
      "Len :(35908, 4000)\n",
      "Feat : 4000\tScore :0.616507017153\t f1 : 0.713155044572\t ROC : 0.843869162158\n",
      "Len :(35908, 5000)\n",
      "Feat : 5000\tScore :0.617843617732\t f1 : 0.715104209914\t ROC : 0.846947999694\n",
      "Len :(35908, 6000)\n",
      "Feat : 6000\tScore :0.617063934061\t f1 : 0.715585704831\t ROC : 0.847944048944\n",
      "Len :(35908, 7000)\n",
      "Feat : 7000\tScore :0.616507017153\t f1 : 0.715853759181\t ROC : 0.848032232663\n",
      "Len :(35908, 8000)\n",
      "Feat : 8000\tScore :0.615504566719\t f1 : 0.715791206982\t ROC : 0.848037666606\n",
      "Len :(35908, 9000)\n",
      "Feat : 9000\tScore :0.615281799955\t f1 : 0.715907221582\t ROC : 0.848049502181\n",
      "Len :(35908, 10000)\n",
      "Feat : 10000\tScore :0.615393183337\t f1 : 0.715779076467\t ROC : 0.848059228692\n",
      "Len :(35908, 11000)\n",
      "Feat : 11000\tScore :0.615393183337\t f1 : 0.715638639545\t ROC : 0.84834137193\n",
      "Len :(35908, 12000)\n",
      "Feat : 12000\tScore :0.615281799955\t f1 : 0.715345310697\t ROC : 0.848129597017\n",
      "Len :(35908, 13000)\n",
      "Feat : 13000\tScore :0.615393183337\t f1 : 0.715451174289\t ROC : 0.848340751617\n",
      "Len :(35908, 14000)\n",
      "Feat : 14000\tScore :0.615170416574\t f1 : 0.715051546392\t ROC : 0.848198029965\n",
      "Len :(35908, 15000)\n",
      "Feat : 15000\tScore :0.615504566719\t f1 : 0.714757891258\t ROC : 0.847662848581\n",
      "Len :(35908, 16000)\n",
      "Feat : 16000\tScore :0.616618400535\t f1 : 0.715301902399\t ROC : 0.848094586542\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k should be >=0, <= n_features; got 17000.Use k='all' to return all features.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3595812b69e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mwords_train_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchi2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Len :\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_train_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petr.lorenc/anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petr.lorenc/anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    327\u001b[0m                             % (self.score_func, type(self.score_func)))\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mscore_func_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petr.lorenc/anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    472\u001b[0m             raise ValueError(\"k should be >=0, <= n_features; got %r.\"\n\u001b[1;32m    473\u001b[0m                              \u001b[0;34m\"Use k='all' to return all features.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                              % self.k)\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: k should be >=0, <= n_features; got 17000.Use k='all' to return all features."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# testing chi square features selection \n",
    "with open('./czech_stop_words.txt') as stop_words:\n",
    "    stop_word_list = list([f.strip() for f in stop_words])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feats = []\n",
    "scores = []\n",
    "f1s = []\n",
    "rocs = []\n",
    "\n",
    "for featureSelected in range(1000, 22000, 1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(words, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "    X_train = count_vect.fit_transform(X_train)\n",
    "    #chi2 = SelectKBest(chi2, k=featureSelected)\n",
    "    #chi2 = SelectKBest(mutual_info_classif, k=featureSelected)\n",
    "    chi2 = SelectKBest(f_classif, k=featureSelected)\n",
    "\n",
    "        \n",
    "    \n",
    "    words_train_count = chi2.fit_transform(X_train, y_train)\n",
    "    \n",
    "    print (\"Len :\" + str(words_train_count.shape))\n",
    "    \n",
    "    X_test = count_vect.transform(X_test)\n",
    "    X_test = chi2.transform(X_test)\n",
    "\n",
    "    tf_transformer = TfidfTransformer(use_idf=True).fit(words_train_count)\n",
    "    words_train_tf = tf_transformer.transform(words_train_count)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    words_train_tfidf = tfidf_transformer.fit_transform(words_train_tf)\n",
    "\n",
    "    clf = MultinomialNB().fit(words_train_tfidf, y_train)\n",
    "\n",
    "    score = clf.score(X_test, y_test)\n",
    "    predicted = clf.predict(X_test)\n",
    "    predicted_proba = clf.predict_proba(X_test)\n",
    "    \n",
    "    \n",
    "    f1 = f1_score(y_pred=predicted, y_true=y_test, average=\"binary\")\n",
    "    roc = roc_auc_score(y_test, predicted_proba[:, 1])\n",
    "\n",
    "    feats.append(featureSelected)\n",
    "    scores.append(score)\n",
    "    f1s.append(f1)\n",
    "    rocs.append(roc)\n",
    "    \n",
    "    print(\"Feat : \" + str(featureSelected) + \"\\tScore :\" + str(score) + \"\\t\" + \" f1 : \" + str(f1) + \"\\t ROC : \" + str(roc))\n",
    "    \n",
    "for x in range(len(feats)):\n",
    "    print(feats[x])\n",
    "\n",
    "print()\n",
    "for x in range(len(scores)):\n",
    "    print(scores[x])\n",
    "\n",
    "print()\n",
    "for x in range(len(f1s)):\n",
    "    print(f1s[x])\n",
    "print()\n",
    "    \n",
    "for x in range(len(rocs)):\n",
    "    print(rocs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
