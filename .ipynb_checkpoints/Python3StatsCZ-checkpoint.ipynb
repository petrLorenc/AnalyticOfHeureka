{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('./json/file_new.json') as data_file:\n",
    "    dataNormal = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '28.\\xa0listopadu\\xa02016',\n",
       " 'minus': ['velmi stručný návod, ale ovládání je intuitivní'],\n",
       " 'nameShop': 'Levenhuk.cz',\n",
       " 'plus': ['předčil očekávání, skvělé jako první mikroskop pro nejmenší děti, dárek pro neteř 4 roky - nutná asistence dospělé osoby, starší děti cca 5-6 let budou používat samy'],\n",
       " 'rating': '100%',\n",
       " 'review': 'předčil očekávání, skvělé jako první mikroskop pro nejmenší děti, dárek pro neteř 4 roky - nutná asistence dospělé osoby, starší děti cca 5-6 let budou používat samy Sami jsme byli velmi zaujati pozorováním, na nejnižší rozlišení 100x krasně vidět struktura listu, ... na větší rozlišení musíme najít vhodné preparáty pro pozorování jednoduchá funkce rozsvícení spodní diody otočením, vhodné pro děti - intuitivní ovládání',\n",
       " 'url': 'http://mikroskopy.heureka.cz/levenhuk-labzz-m1/recenze/',\n",
       " 'usefulReview': 1,\n",
       " 'uselessReview': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataNormal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load right dataset\n",
    "dataIteration = dataNormal\n",
    "\n",
    "def filterNonEnglishOut(dataIteration):\n",
    "    dataIteration_preprocess = []\n",
    "\n",
    "    words_order = dataIteration\n",
    "    only_english_words = []\n",
    "\n",
    "    # to keep only english words\n",
    "    with open(\"./wordlist_cz.txt\") as word_file:\n",
    "        only_english_words = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "    for line in words_order:\n",
    "        english_line = \"\"\n",
    "        for w in line.split():\n",
    "            if w.strip().lower() in english_words_file:\n",
    "                english_line += str(w.strip().lower()) + \" \"\n",
    "            else: \n",
    "                pass\n",
    "        dataIteration_preprocess.append(english_line)    \n",
    "    return dataIteration_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plusWords = \"\"\n",
    "minusWords = \"\"\n",
    "\n",
    "words = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "#every review\n",
    "for x in range(0,len(dataIteration)):\n",
    "        \n",
    "    for plus in  dataIteration[x].get(\"plus\"):\n",
    "        plusWords += str(plus) + \" \"\n",
    "        \n",
    "    words.append(plus)\n",
    "    labels.append(1)\n",
    "\n",
    "    for minus in dataIteration[x].get(\"minus\"):\n",
    "        minusWords += str(minus) + \" \"\n",
    "        \n",
    "    words.append(minus)\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(words, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "with open('./czech_stop_words.txt') as stop_words:\n",
    "    stop_word_list = list([f.strip() for f in stop_words])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer( stop_words=stop_word_list)\n",
    "\n",
    "words_train_count = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35912, 21161)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21161"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (count_vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs = [(word, words_train_count.getcol(idx).sum()) for word, idx in count_vect.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "print (sorted (freqs, key = lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(words_train_count)\n",
    "words_train_tf = tf_transformer.transform(words_train_count)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "words_train_tfidf = tfidf_transformer.fit_transform(words_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(words_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_new = X_test\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "\n",
    "X_new_counts = SelectKBest(chi2, k=15).fit_transform(X_new_counts, y_test)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Score \" + str(clf.score(X_new_tfidf, y_test)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "# F1 = 2 * (precision * recall) / (precision + recall) \n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"binary\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"macro\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"micro\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"weighted\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "predicted_proba = clf.predict_proba(X_new_tfidf)\n",
    "# Compute Area Under the Curve (AUC) from prediction scores\n",
    "print (\"ROC :\" + str(roc_auc_score(y_test, predicted_proba[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(words, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Sample pipeline for text feature extraction and evaluation\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    #('feature_selection', SelectFromModel(MultinomialNB())),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "with open('./czech_stop_words.txt') as stop_words:\n",
    "    stop_word_list = list([f.strip() for f in stop_words])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "#     'vect__max_df': (0.5, 0.75, 1.0),\n",
    "#     'vect__stop_words' : (stop_word_list ,'english', None),\n",
    "    #'vect__max_features': (None,1000,2000,3000,4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000),\n",
    "#     'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "#     'vect__analyzer': ('word', 'char', 'char_wb') ,\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'tfidf__norm': ('l1', 'l2'),\n",
    "    #'feature_selection__estimator' : (MultinomialNB, )\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "# multiprocessing requires the fork to happen in a __main__ protected\n",
    "# block\n",
    "\n",
    "# find the best parameters for both the feature extraction and the\n",
    "# classifier\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=3)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_test, y_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feat : 1000\tScore :0.86700824237\t f1 : 0.860154602952\t ROC : 0.926465186517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# testing chi square features selection \n",
    "with open('./czech_stop_words.txt') as stop_words:\n",
    "    stop_word_list = list([f.strip() for f in stop_words])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count_vect = CountVectorizer( stop_words=stop_word_list)\n",
    "\n",
    "feats = []\n",
    "scores = []\n",
    "f1s = []\n",
    "rocs = []\n",
    "\n",
    "for featureSelected in range(1000, 22000, 1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(words, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "    X_train = count_vect.fit_transform(X_train)\n",
    "    chi2 = SelectKBest(mutual_info_classif, k=featureSelected)\n",
    "    words_train_count = chi2.fit_transform(X_train, y_train)\n",
    "    \n",
    "    X_test = count_vect.transform(X_test)\n",
    "    X_test = chi2.transform(X_test)\n",
    "    \n",
    "    feature_names = count_vect.get_feature_names()\n",
    "    if feature_names:\n",
    "        # keep selected feature names\n",
    "        feature_names = [feature_names[i] for i\n",
    "                         in chi2.get_support(indices=True)]\n",
    "\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(words_train_count)\n",
    "    words_train_tf = tf_transformer.transform(words_train_count)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    words_train_tfidf = tfidf_transformer.fit_transform(words_train_tf)\n",
    "\n",
    "    clf = MultinomialNB().fit(words_train_tfidf, y_train)\n",
    "\n",
    "#     docs_new = X_test\n",
    "#     X_new_counts = count_vect.transform(docs_new)\n",
    "\n",
    "#     X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    predicted = clf.predict(X_test)\n",
    "    predicted_proba = clf.predict_proba(X_test)\n",
    "    \n",
    "    \n",
    "    f1 = f1_score(y_pred=predicted, y_true=y_test, average=\"binary\")\n",
    "    roc = roc_auc_score(y_test, predicted_proba[:, 1])\n",
    "\n",
    "    feats.append(featureSelected)\n",
    "    scores.append(score)\n",
    "    f1s.append(f1)\n",
    "    rocs.append(roc)\n",
    "    \n",
    "    print(\"Feat : \" + str(featureSelected) + \"\\tScore :\" + str(score) + \"\\t\" + \" f1 : \" + str(f1) + \"\\t ROC : \" + str(roc))\n",
    "    \n",
    "for x in range(len(feats)):\n",
    "    print(feats[x])\n",
    "\n",
    "print()\n",
    "for x in range(len(scores)):\n",
    "    print(scores[x])\n",
    "\n",
    "print()\n",
    "for x in range(len(f1s)):\n",
    "    print(f1s[x])\n",
    "print()\n",
    "    \n",
    "for x in range(len(rocs)):\n",
    "    print(rocs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
