{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./json/json_translated.json') as data_file:\n",
    "    dataTranslatedNormal = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22445"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataTranslatedNormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load right dataset\n",
    "dataIteration = dataTranslatedNormal\n",
    "\n",
    "def filterNonEnglishOut(dataIteration):\n",
    "    dataIteration_preprocess = []\n",
    "\n",
    "    words_order = dataIteration\n",
    "    only_english_words = []\n",
    "\n",
    "    # to keep only english words\n",
    "    with open(\"./wordlist_en.txt\") as word_file:\n",
    "        only_english_words = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "    for line in words_order:\n",
    "        english_line = \"\"\n",
    "        for w in line.split():\n",
    "            if w.strip().lower() in english_words_file:\n",
    "                english_line += str(w.strip().lower()) + \" \"\n",
    "            else: \n",
    "                pass\n",
    "        dataIteration_preprocess.append(english_line)    \n",
    "    return dataIteration_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exceeded expectations, great as the first microscope for the youngest children, a gift for a niece four years - needed assistance to adults, older children 5-6 years old will use themselves']\n",
      "['great as the first microscope for the a gift for a niece four years needed assistance to older children years old will use themselves ']\n"
     ]
    }
   ],
   "source": [
    "print (dataIteration[0].get(\"plus\"))\n",
    "print (filterNonEnglishOut(dataIteration=dataIteration[0].get(\"plus\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plusWords = []\n",
    "minusWords = []\n",
    "\n",
    "words = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "#every review\n",
    "for x in range(0,len(dataIteration)):\n",
    "    \n",
    "    for plus in  dataIteration[x].get(\"plus\"):\n",
    "        plusWords.append(plus) # only for informative purpose\n",
    "        \n",
    "        words.append(plus)\n",
    "        labels.append(1)\n",
    "\n",
    "    for minus in dataIteration[x].get(\"minus\"):\n",
    "        minusWords.append(minus) # only for informative purpose\n",
    "        \n",
    "        words.append(minus)\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(words, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "words_train_count = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 7260), ('and', 4307), ('of', 3483), ('not', 3398), ('to', 2980), ('price', 2480), ('it', 2375), ('is', 2308), ('nothing', 2182), ('for', 1987)]\n"
     ]
    }
   ],
   "source": [
    "freqs = [(word, words_train_count.getcol(idx).sum()) for word, idx in count_vect.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "print (sorted (freqs, key = lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(words_train_count)\n",
    "words_train_tf = tf_transformer.transform(words_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "words_train_tfidf = tfidf_transformer.fit_transform(words_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(words_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'clever tool' => 1\n",
      "'bad experience' => 0\n",
      "'very poor instruction' => 0\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['clever tool', 'bad experience', 'very poor instruction']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "     print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.875124448869\n",
      "0.911971124925\n",
      "0.848598086924\n",
      "0.875124448869\n",
      "0.870870135978\n",
      "[ 0.78522505  0.91197112]\n"
     ]
    }
   ],
   "source": [
    "docs_new = X_test\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "print(\"Score \" + str(clf.score(X_new_tfidf, y_test)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "# F1 = 2 * (precision * recall) / (precision + recall) \n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"binary\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"macro\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"micro\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"weighted\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC :0.948830070862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "predicted_proba = clf.predict_proba(X_new_tfidf)\n",
    "# Compute Area Under the Curve (AUC) from prediction scores\n",
    "print (\"ROC :\" + str(roc_auc_score(y_test, predicted_proba[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__use_idf': (True, False),\n",
      " 'vect__analyzer': ('word', 'char', 'char_wb'),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__max_features': (None, 5000, 10000, 50000),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2)),\n",
      " 'vect__stop_words': (['a',\n",
      "                       'about',\n",
      "                       'above',\n",
      "                       'after',\n",
      "                       'again',\n",
      "                       'against',\n",
      "                       'all',\n",
      "                       'am',\n",
      "                       'an',\n",
      "                       'and',\n",
      "                       'any',\n",
      "                       'are',\n",
      "                       \"aren't\",\n",
      "                       'as',\n",
      "                       'at',\n",
      "                       'be',\n",
      "                       'because',\n",
      "                       'been',\n",
      "                       'before',\n",
      "                       'being',\n",
      "                       'below',\n",
      "                       'between',\n",
      "                       'both',\n",
      "                       'but',\n",
      "                       'by',\n",
      "                       \"can't\",\n",
      "                       'cannot',\n",
      "                       'could',\n",
      "                       \"couldn't\",\n",
      "                       'did',\n",
      "                       \"didn't\",\n",
      "                       'do',\n",
      "                       'does',\n",
      "                       \"doesn't\",\n",
      "                       'doing',\n",
      "                       \"don't\",\n",
      "                       'down',\n",
      "                       'during',\n",
      "                       'each',\n",
      "                       'few',\n",
      "                       'for',\n",
      "                       'from',\n",
      "                       'further',\n",
      "                       'had',\n",
      "                       \"hadn't\",\n",
      "                       'has',\n",
      "                       \"hasn't\",\n",
      "                       'have',\n",
      "                       \"haven't\",\n",
      "                       'having',\n",
      "                       'he',\n",
      "                       \"he'd\",\n",
      "                       \"he'll\",\n",
      "                       \"he's\",\n",
      "                       'her',\n",
      "                       'here',\n",
      "                       \"here's\",\n",
      "                       'hers',\n",
      "                       'herself',\n",
      "                       'him',\n",
      "                       'himself',\n",
      "                       'his',\n",
      "                       'how',\n",
      "                       \"how's\",\n",
      "                       'i',\n",
      "                       \"i'd\",\n",
      "                       \"i'll\",\n",
      "                       \"i'm\",\n",
      "                       \"i've\",\n",
      "                       'if',\n",
      "                       'in',\n",
      "                       'into',\n",
      "                       'is',\n",
      "                       \"isn't\",\n",
      "                       'it',\n",
      "                       \"it's\",\n",
      "                       'its',\n",
      "                       'itself',\n",
      "                       \"let's\",\n",
      "                       'me',\n",
      "                       'more',\n",
      "                       'most',\n",
      "                       \"mustn't\",\n",
      "                       'my',\n",
      "                       'myself',\n",
      "                       'no',\n",
      "                       'nor',\n",
      "                       'not',\n",
      "                       'of',\n",
      "                       'off',\n",
      "                       'on',\n",
      "                       'once',\n",
      "                       'only',\n",
      "                       'or',\n",
      "                       'other',\n",
      "                       'ought',\n",
      "                       'our',\n",
      "                       'ours\\tourselves',\n",
      "                       'out',\n",
      "                       'over',\n",
      "                       'own',\n",
      "                       'same',\n",
      "                       \"shan't\",\n",
      "                       'she',\n",
      "                       \"she'd\",\n",
      "                       \"she'll\",\n",
      "                       \"she's\",\n",
      "                       'should',\n",
      "                       \"shouldn't\",\n",
      "                       'so',\n",
      "                       'some',\n",
      "                       'such',\n",
      "                       'than',\n",
      "                       'that',\n",
      "                       \"that's\",\n",
      "                       'the',\n",
      "                       'their',\n",
      "                       'theirs',\n",
      "                       'them',\n",
      "                       'themselves',\n",
      "                       'then',\n",
      "                       'there',\n",
      "                       \"there's\",\n",
      "                       'these',\n",
      "                       'they',\n",
      "                       \"they'd\",\n",
      "                       \"they'll\",\n",
      "                       \"they're\",\n",
      "                       \"they've\",\n",
      "                       'this',\n",
      "                       'those',\n",
      "                       'through',\n",
      "                       'to',\n",
      "                       'too',\n",
      "                       'under',\n",
      "                       'until',\n",
      "                       'up',\n",
      "                       'very',\n",
      "                       'was',\n",
      "                       \"wasn't\",\n",
      "                       'we',\n",
      "                       \"we'd\",\n",
      "                       \"we'll\",\n",
      "                       \"we're\",\n",
      "                       \"we've\",\n",
      "                       'were',\n",
      "                       \"weren't\",\n",
      "                       'what',\n",
      "                       \"what's\",\n",
      "                       'when',\n",
      "                       \"when's\",\n",
      "                       'where',\n",
      "                       \"where's\",\n",
      "                       'which',\n",
      "                       'while',\n",
      "                       'who',\n",
      "                       \"who's\",\n",
      "                       'whom',\n",
      "                       'why',\n",
      "                       \"why's\",\n",
      "                       'with',\n",
      "                       \"won't\",\n",
      "                       'would',\n",
      "                       \"wouldn't\",\n",
      "                       'you',\n",
      "                       \"you'd\",\n",
      "                       \"you'll\",\n",
      "                       \"you're\",\n",
      "                       \"you've\",\n",
      "                       'your',\n",
      "                       'yours',\n",
      "                       'yourself',\n",
      "                       'yourselves'],\n",
      "                      'english',\n",
      "                      None)}\n",
      "Fitting 3 folds for each of 864 candidates, totalling 2592 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2592 out of 2592 | elapsed: 12.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 744.044s\n",
      "\n",
      "Best score: 0.861\n",
      "Best parameters set:\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: 5000\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__stop_words: None\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample pipeline for text feature extraction and evaluation\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "with open('./english_stop_words.txt') as stop_words:\n",
    "    stop_word_list = list([f.strip() for f in stop_words])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__stop_words' : (stop_word_list ,'english', None),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'vect__analyzer': ('word', 'char', 'char_wb') ,\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "# multiprocessing requires the fork to happen in a __main__ protected\n",
    "# block\n",
    "\n",
    "# find the best parameters for both the feature extraction and the\n",
    "# classifier\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_test, y_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
