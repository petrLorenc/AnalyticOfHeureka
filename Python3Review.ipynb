{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('./json/file_new_stopwords_token.json') as data_file:\n",
    "    dataNormal = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load right dataset\n",
    "dataIteration = dataNormal\n",
    "\n",
    "def filterNonEnglishOut(dataIteration):\n",
    "    dataIteration_preprocess = []\n",
    "\n",
    "    words_order = dataIteration\n",
    "    only_english_words = []\n",
    "\n",
    "    # to keep only english words\n",
    "    with open(\"./wordlist_cz.txt\") as word_file:\n",
    "        only_english_words = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "    for line in words_order:\n",
    "        english_line = \"\"\n",
    "        for w in line.split():\n",
    "            if w.strip().lower() in english_words_file:\n",
    "                english_line += str(w.strip().lower()) + \" \"\n",
    "            else: \n",
    "                pass\n",
    "        dataIteration_preprocess.append(english_line)    \n",
    "    return dataIteration_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviewWords = \"\"\n",
    "\n",
    "words = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "treashold = 1\n",
    "\n",
    "passCounter = 0\n",
    "\n",
    "#every review\n",
    "for x in range(0,len(dataIteration)):\n",
    "    \n",
    "    reviewWords = \"\"\n",
    "    \n",
    "    for review in  dataIteration[x].get(\"review\"):\n",
    "        reviewWords += review + \" \"\n",
    "        \n",
    "\n",
    "    if(dataIteration[x].get(\"usefulReview\") > dataIteration[x].get(\"uselessReview\") + treashold):\n",
    "        labels.append(1)\n",
    "        words.append(reviewWords)\n",
    "#         labels.append(1)\n",
    "#         words.append(reviewWords)\n",
    "#         labels.append(1)\n",
    "#         words.append(reviewWords)\n",
    "#         labels.append(1)\n",
    "#         words.append(reviewWords)\n",
    "#         labels.append(1)\n",
    "#         words.append(reviewWords)\n",
    "#         labels.append(1)\n",
    "#         words.append(reviewWords)\n",
    "#         labels.append(1)\n",
    "#         words.append(reviewWords)\n",
    "    elif (dataIteration[x].get(\"usefulReview\") - treashold < dataIteration[x].get(\"uselessReview\")):\n",
    "        if(x % 5 == 0):\n",
    "            labels.append(-1)\n",
    "            words.append(reviewWords)\n",
    "    else :\n",
    "        passCounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(words, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "words_train_count = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10753"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (count_vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('doporučovat', 3135), ('dobrý', 2518), ('cena', 1898), ('spokojený', 1828), ('obchod', 1443), ('zboží', 1288), ('velmi', 1127), ('dno', 1121), ('dna', 1069), ('rychlý', 1023)]\n"
     ]
    }
   ],
   "source": [
    "freqs = [(word, words_train_count.getcol(idx).sum()) for word, idx in count_vect.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "print (sorted (freqs, key = lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(words_train_count)\n",
    "words_train_tf = tf_transformer.transform(words_train_count)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "words_train_tfidf = tfidf_transformer.fit_transform(words_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16236, 18471)\n",
      "(16236, 13248)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(words_train_tfidf, y_train)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# Set a minimum threshold of 0.25\n",
    "\n",
    "sfm = SelectFromModel(clf , prefit=True)\n",
    "print (words_train_tfidf.shape)\n",
    "words_train_tfidf = sfm.transform(words_train_tfidf)\n",
    "print (words_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19221"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.734939759036\n",
      "0.567795518207\n",
      "0.734939759036\n",
      "0.674160035098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(words_train_tfidf, y_train)\n",
    "docs_new = X_test\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "print(\"Score \" + str(clf.score(X_new_tfidf, y_test)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "# F1 = 2 * (precision * recall) / (precision + recall) \n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"macro\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"micro\"))\n",
    "print (f1_score(y_pred=predicted, y_true=y_test, average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC :0.727250507174\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "predicted_proba = clf.predict_proba(X_new_tfidf)\n",
    "# Compute Area Under the Curve (AUC) from prediction scores\n",
    "print (\"ROC :\" + str(roc_auc_score(y_test, predicted_proba[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3078064157240868\n",
      "3733\n",
      "1660\n"
     ]
    }
   ],
   "source": [
    "print (len([l for l in labels if l == 1])/len(labels))\n",
    "print (len([l for l in labels if l == -1]))\n",
    "print (len([l for l in labels if l == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample pipeline for text feature extraction and evaluation\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "with open('./czech_stop_words.txt') as stop_words:\n",
    "    stop_word_list = list([f.strip() for f in stop_words])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__stop_words' : (stop_word_list ,'english', None),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'vect__analyzer': ('word', 'char', 'char_wb') ,\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "# multiprocessing requires the fork to happen in a __main__ protected\n",
    "# block\n",
    "\n",
    "# find the best parameters for both the feature extraction and the\n",
    "# classifier\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_test, y_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
